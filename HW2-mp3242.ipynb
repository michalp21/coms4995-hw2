{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# COMS 4995_002 Deep Learning Assignment 2\n",
    "Due on Thursday, Feb 27, 11:59pm\n",
    "\n",
    "This assignment can be done in groups of at most 2 students. Everyone must submit on Courseworks individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Member 1: Michal Porubcin, mp3242\n",
    "\n",
    "Member 2: Daniel Echlin, dje2126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.data import Dataset, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = .001\n",
    "MAX_ITER = 1000\n",
    "BATCH_SIZE = 100\n",
    "DROP_PROB = 0 #0.0-1.0\n",
    "PERCENT_VALID = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CNN Code here ####\n",
    "def train(X_trn, y_trn, X_val, y_val):\n",
    "    \"\"\"\n",
    "    :param X: input samples, each column is a sample\n",
    "    :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "    \"\"\"\n",
    "\n",
    "    # Input placeholders, None indicates variable size (for batches)\n",
    "#     with tf.name_scope('input'):\n",
    "    X_p = tf.placeholder(tf.float32)\n",
    "    y_p = tf.placeholder(tf.float32, shape=(None), name='y-input')\n",
    "#     with tf.name_scope('input_reshape'):\n",
    "#         image_shaped_input = tf.reshape(x, [-1, 32, 32, 3])\n",
    "#         tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "\n",
    "    # get minibatch\n",
    "    #X_batch, y_batch = get_batch(X, y, batch_size)\n",
    "    \n",
    "    # dense layer\n",
    "    hidden_1 = tf.layers.dense(X_p, units=500, activation=tf.nn.relu)\n",
    "    variable_summaries(hidden_1.kernel)\n",
    "    \n",
    "    # predictions\n",
    "    y_hat = tf.layers.Dense(inputs=hidden_1,units=10,activation=tf.identity)\n",
    "    variable_summaries(y_hat.kernel)\n",
    "    tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_p, logits=y_hat)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_hat, 1), y_p)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to cifar10-hw2 (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('logs/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logs/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    def feed_dict(isTrain):\n",
    "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "        if isTrain:\n",
    "            xs, ys = get_batch(X_trn,y_trn,BATCH_SIZE)\n",
    "            k = 1.0 - DROP_PROB\n",
    "            return {x: xs, y_: ys, keep_prob: k}\n",
    "        else:\n",
    "            xs, ys = get_batch(X_val,y_val,BATCH_SIZE)\n",
    "            k = 1.0 - DROP_PROB\n",
    "            return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "    for i in range(MAX_ITER):\n",
    "        if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "        else:  # Record train set summaries, and train\n",
    "            if i % 100 == 99:  # Record execution stats\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                      feed_dict=feed_dict(True),\n",
    "                                      run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    \n",
    "def get_batch(self, X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Return minibatch of samples and labels\n",
    "\n",
    "    :param X, y: samples and corresponding labels\n",
    "    :parma batch_size: minibatch size\n",
    "    :returns: (tuple) X_batch, y_batch\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[1]\n",
    "    indices = np.random.randint(num_samples, size=batch_size)\n",
    "    X_batch = X[:, indices]\n",
    "    y_batch = y[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def resize_img(img_arr, target_dim=(50, 50)):\n",
    "    \"\"\"\n",
    "    Resizes img represented as numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imresize(img_arr, target_dim)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)\n",
    "\n",
    "# Functions to load data\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "\n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "#         if count > 100:\n",
    "#             break\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "#     X = tf.map_fn(lambda image: tf.image.per_image_standardization(image), images, tf.int32)\n",
    "    X = np.column_stack(images)\n",
    "#     X = images\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path, suffix=\"train\"):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + suffix\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/traes/Documents/15UG/Sem2/DeepLearning/Assignments/HW2/venv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Train data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "data_root_path = './cifar10-hw2/'\n",
    "# data_root_path = './cifar10-simple/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "# X_centered = (X_train - np.mean(X_train, axis=1)[:, np.newaxis])\n",
    "# X_div = (X_centered / np.std(X_centered, axis=1)[:, np.newaxis])\n",
    "# X_train = X_div\n",
    "print('Train data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/traes/Documents/15UG/Sem2/DeepLearning/Assignments/HW2/venv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000/10000\n",
      "Test data loading done\n"
     ]
    }
   ],
   "source": [
    "#Load test data\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "# X_centered = (X_test - np.mean(X_test, axis=1)[:, np.newaxis])\n",
    "# X_div = (X_centered / np.std(X_centered, axis=1)[:, np.newaxis])\n",
    "# X_test = X_div\n",
    "print('Test data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "shuffle_in_unison(X_train, y_train)\n",
    "X_val_index = int(X_train.shape[1]/PERCENT_VALID)\n",
    "y_val_index = int(len(y_train)/PERCENT_VALID)\n",
    "\n",
    "X_val, X_trn = X_train[:,:X_val_index], X_train[:,X_val_index:]\n",
    "y_val, y_trn = y_train[:y_val_index], y_train[y_val_index:]\n",
    "\n",
    "# trn_dataset = tf.data.Dataset.from_tensor_slices((X_trn,y_trn)).shuffle(buffer_size=len(y_trn)).repeat().batch(BATCH_SIZE)\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val,y_val))#.shuffle().batch(BATCH_SIZE)\n",
    "\n",
    "# handle = tf.placeholder(tf.string, shape=[])\n",
    "# #can use either training or validation shapes since identical\n",
    "# iterator = tf.data.Iterator.from_string_handle(\n",
    "#     handle, trn_dataset.output_types, trn_dataset.output_shapes)\n",
    "# next_element = iterator.get_next()\n",
    "\n",
    "# trn_iter = trn_dataset.make_one_shot_iterator()\n",
    "# trn_handle = trn_iter.string_handle()\n",
    "\n",
    "# val_iter = val_dataset.make_one_shot_iterator()\n",
    "# val_handle = val_iter.string_handle()\n",
    "\n",
    "# print(\"ok he11o\")\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     trn_string = sess.run(trn_handle)\n",
    "#     val_string = sess.run(tst_handle)\n",
    "#     for i in range(2):\n",
    "#         print(sess.run(next_element, feed_dict={handle: trn_string}))\n",
    "\n",
    "#         print(sess.run(next_element, feed_dict={handle: val_string}))\n",
    "\n",
    "##################\n",
    "\n",
    "# myiter = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "# train_init_op = myiter.make_initializer(train_dataset)\n",
    "# test_init_op = myiter.make_initializer(test_dataset)\n",
    "# features, labels = myiter.get_next() #REMEMBER: lazy eval\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     for _ in range(2):\n",
    "#     sess.run(train_init_op) # switch to train dataset\n",
    "#     for _ in range(EPOCHS):\n",
    "#         sess.run([features, labels])\n",
    "#     sess.run(test_init_op) # switch to val dataset\n",
    "#     print(sess.run([features, labels]))\n",
    "\n",
    "#Feed Data\n",
    "# test_features = tf.placeholder(X_test.dtype, X_test.shape)\n",
    "# test_labels = tf.placeholder(y_test.dtype, y_test.shape)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "# train_features = tf.placeholder(X_train.dtype, X_train.shape)\n",
    "# train_labels = tf.placeholder(y_train.dtype, y_train.shape)\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "# sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer dense_2 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1ebce298441a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# layer_dimensions = [X_train.shape[0], 500, 500, 500, 10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#CNN.train(X_train, y_train, iters=2500, alpha=0.0005, batch_size=200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f73a7dd7e5a1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_trn, y_trn, X_val, y_val)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# dense layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mhidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mvariable_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/15UG/Sem2/DeepLearning/Assignments/HW2/venv/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 253\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/15UG/Sem2/DeepLearning/Assignments/HW2/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \"\"\"\n\u001b[0;32m--> 762\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m~/Documents/15UG/Sem2/DeepLearning/Assignments/HW2/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m           \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/15UG/Sem2/DeepLearning/Assignments/HW2/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0;32m-> 1122\u001b[0;31m                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m                            \u001b[0;34m'its rank is undefined, but the layer requires a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m                            'defined rank.')\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer dense_2 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank."
     ]
    }
   ],
   "source": [
    "#Run CNN\n",
    "# layer_dimensions = [X_train.shape[0], 500, 500, 500, 10]\n",
    "train(X_trn,y_trn,X_val,y_val)\n",
    "#CNN.train(X_train, y_train, iters=2500, alpha=0.0005, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "y_predicted, cache = NN.predict(X_test)\n",
    "save_predictions('ans1-mp3242', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-mp3242.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write down results here:\n",
    "\n",
    "Below is our setup:\n",
    "- Hidden Layers: [1536]\n",
    "- alpha: .0005, decay factor: .83 (manually set on line 384)\n",
    "- Batch size: 200\n",
    "- 2500 iterations\n",
    "\n",
    "We got about xx% validation accuracy with this run.\n",
    "\n",
    "Blablabla"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
